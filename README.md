
Generating Star Trek scripts with GPT-2
Author: Fennec C. Nightingale

Overview
Using python to generate life-like text 

Business Problem
Writing hard, 


Data
Over the last 50+ years roughly 400 hours of Star Trek have aired, for this project I gathered every script available online, reformatted them so that all of the lines read the same, and broke them up into 4 roughly equal parts after personally determining when scenes have ended. 

This required sourcing scripts from a variety of sites online: 
https://www.st-minutiae.com/resources/scripts/
http://www.chakoteya.net/StarTrek/
https://scifi.media/star-trek/transcripts/
https://subslikescript.com/ (Names not listed, had to rewatch all recent series and hand enter names as people spoke)
https://scrapsfromtheloft.com/ (Names partially listed, had to rewatch all recent series and hand enter names)

Methods

Results

Modeling

Conclusions
This analysis leads to three recommendations:

ONE. 
TWO. 
THREE. 

Future Work
Train on newer and/or larger models (GPT-3/1558M)
Add more scripts, wether branching out into more general Sci-Fi or waiting for more Star Trek. 
Exlpore the scripts in more fun ways to learn more about the series. 

For More Information
See the full analysis in our Jupyter Notebook or review my Presentation.

For additional info, contact me at: fenneccharles@gmail.com

Repository Structure

├──.ipynb_checkpoints
├──.gitignore
├──data
    ├─
├──Images
    ├── 
├── pdfs
    ├──
├──
├── README.ipynb
└──
