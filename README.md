
Generating Star Trek scripts with GPT-2<br>
Author: Fennec C. Nightingale<br>

Overview
Using python to generate life-like text <br>

Business Problem
Writing hard, <br>


Data
Over the last 50+ years roughly 400 hours of Star Trek have aired, for this project I gathered every script available online, reformatted them so that all of the lines read the same, and broke them up into 4 roughly equal parts after personally determining when scenes have ended. <br>

This required sourcing scripts from a variety of sites online: <br>
https://www.st-minutiae.com/resources/scripts/<br>
http://www.chakoteya.net/StarTrek/<br>
https://scifi.media/star-trek/transcripts/<br>
https://subslikescript.com/ (Names not listed, had to rewatch all recent series and hand enter names as people spoke)<br>
https://scrapsfromtheloft.com/ (Names partially listed, had to rewatch all recent series and hand enter names)<br>

Methods<br>

Results<br>

Modeling<br>

Conclusions
This analysis leads to three recommendations:<br>

ONE. <br>
TWO. <br>
THREE. <br>

Future Work
Train on newer and/or larger models (GPT-3/1558M)
Add more scripts, wether branching out into more general Sci-Fi or waiting for more Star Trek. 
Exlpore the scripts in more fun ways to learn more about the series. <br>

For More Information
See the full analysis in our Jupyter Notebook or review my Presentation.<br>

For additional info, contact me at: fenneccharles@gmail.com<br>

Repository Structure

├──.ipynb_checkpoints
├──.gitignore
├──data
    ├─
├──Images
    ├── 
├── pdfs
    ├──
├──
├── README.ipynb
└──
