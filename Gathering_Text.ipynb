{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Gathering_Text",
      "provenance": [],
      "collapsed_sections": [
        "ClJwpF_ACONp",
        "QQAN3M6RT7Kj",
        "ig-KVgkCDCKD",
        "wmTXWNUygS5E"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "64e23f22-1718-4362-c6b8-2f137da1bc18"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import re \n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Mounted at /content/drive/\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKEG-24aGqHT"
      },
      "source": [
        "import tarfile\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import sys\n",
        "import shutil\n",
        "import re\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "from tensorflow.python.client import device_lib\n",
        "import time\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import argparse\n",
        "\n",
        "# if in Google Colaboratory\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except:\n",
        "    pass\n",
        "\n",
        "from gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\n",
        "from gpt_2_simple.src.load_dataset import load_dataset, Sampler\n",
        "from gpt_2_simple.src.accumulate import AccumulatingOptimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUcY9h1f42Sz"
      },
      "source": [
        "names = {\n",
        "  \" ADI \"  :  \" ADIRA TAL \",  \n",
        "  \" BRN \"  :  \" MICHAEL BURNHAM \",\n",
        "  \" BRY \"  :  \" BRYCE \",\n",
        "  \" BK \"   :  \" BOOKER \",\n",
        "  \" COMP \" :  \" COMPUTER \",\n",
        "  \" COS \"  :  \" COSMO \",\n",
        "  \" DET \"  :  \" KEYLA DETMER \",\n",
        "  \" GAB \"  :  \" GABRIELLE BURNHAM \", \n",
        "  \" GRG \"  :  \" PHILLIPA GEORGIOU \",\n",
        "  \" GRY \"  :  \" GREY TAL \",\n",
        "  \" ITH \"  :  \" ITHOR \",\n",
        "  \" TLR \"  :  \" TOLOR \", \n",
        "  \" NIL \"  :  \" NILSSON \", \n",
        "  \" OWO \"  :  \" JOANN OWOSEKUN \",\n",
        "  \" OSS \"  :  \" OSSYRA \", \n",
        "  \" POL \"  :  \" DR.POLLARD \", \n",
        "  \" RHYS \" :  \" GEN RHYS \",\n",
        "  \" SAR \"  :  \" SARU \", \n",
        "  \" SEN \"  :  \" SENNA TAL \", \n",
        "  \" STA \"  :  \" PAUL STAMETS \", \n",
        "  \" TLY \"  :  \" TILLY \", \n",
        "  \" NDY \"  :  \" CAPTAIN NDYOE \",\n",
        "  \" XI \"   :  ' GAURDIAN TRILL ',\n",
        "  \" PAV \"  :  \" LEADER TRILL \",\n",
        "  \" VN \"   :  \" VANCE \",\n",
        "  \" KAR \"  :  \" CAPTAIN KARDASHEV \", \n",
        "  \" RA \"   :  \" CAPTAIN RAHMA \", \n",
        "  \" BAN \"  :  \" CAPTAIN BANDRA \", \n",
        "  \" VK \"   :  \" V'KIR \",\n",
        "  \" NR \"   :  \" N'RAJ \",\n",
        "  \" SHR \"  :  \" SHIRA \",   \n",
        "  \" TR \"   :  \" T'RINA \",\n",
        "  \" ARL \"  :  \" AURELLIO \",\n",
        "  \" CR \"   :  \" CARL \", \n",
        "  \" ISS HOLO \" :  \" SUKALS MOM \", \n",
        "  \" KLY \"  :  \" KILLY \", \n",
        "  \" SKL \"  :  \" SUKAL \", \n",
        "  \" WL \"   :  \" LT WILLA \", \n",
        "  \" OS \"   :  \" OS'IR  BARTENDER \",\n",
        "  \" ZAR \"  :  \" ZAREH \", \n",
        "  \" HLM COMP \" : \" HELMENT COMPUTER \", \n",
        "  \" ADM \"  :  \" ADMIRAL CORNWELL \",\n",
        "  \" ASH \"  :  \" ASH TYLER \", \n",
        "  \" PKE \"  :  \" CHRISTOPHER PIKE \", \n",
        "  \" SPK \"  :  \" SPOCK \", \n",
        "  \" SRK \"  :  \" SAREK \", \n",
        "  \" AMD \"  :  \" AMANDA \", \n",
        "  \" LRL \"  :  \" L'RELL \", \n",
        "  \" LEL \"  :  \" LELAND \",\n",
        "  \" NUM \"  :  \" NUMBER ONE \",\n",
        "  \" SIR \" :   \" SIRANNA \",\n",
        "  \" BRN DAD \" : \" BURNHAMS DAD \", \n",
        "  \" CTRL \" : \" CONTROL \",\n",
        "  \" VIK \"  :  \" TENAVIK \",\n",
        "  \" KAM \"  :  \" KAMARAN GANT \",\n",
        "  \" CONN \" :  \" EVAN CONNOLLY \",\n",
        "  \" GAR \"  :  \" GARRISON \", \n",
        "  \" TAL \"  : \" TALOS MAN \", \n",
        "  \" TLSN \" : \" TALOSIAN \", \n",
        "  \" PSY \"  : \" PYSCH \", \n",
        "  \" SEC ONE \" : \"FIRST GAURD \", \n",
        "  \" SEC TWO \" : \"SECOND GAURD \",\n",
        "  \" THR TRANS \" :  \" SECTION THIRTY-ONE TRANSPORTER \", \n",
        "  \" ARIM \" :  \" AIRIAM \",\n",
        "  \" HUS \" : \" AIRIAM HUSBAND \",\n",
        "  \" PAT \"  :  \" PATAR \", \n",
        "  \" ANDO\"  : \" ANDORAIN ADMIRAL \",\n",
        "  \" TEL \"  :  \" TELLARITE ADMIRAL \", \n",
        "  \" KOL \"  :  \" KOL-SHA \",\n",
        "  \" UJI \"  :  \" UJILI MO'KAI \", \n",
        "  \" CUL HOLO \" : \" HOLOGRAPHIC HUGH CULBER \", \n",
        "  \" JL \"  :  \" PICARD \", \n",
        "  \" FI \"  :  \" RAFFI \",\n",
        "  \" AGN \" :  \" ANGES \",\n",
        "  \" HG \"  :  \" HUGH \", \n",
        "  \" SVN \" :  \" SEVEN \", \n",
        "  \" ICH \" :  \" ICHEB \",\n",
        "  \" NRK \" :  \" NAREK \", \n",
        "  \" SJ \"  :  \" SOJI \",\n",
        "  \" NRS \" :  \" NARISSA \", \n",
        "  \" RK \"  :  \" WILL RIKER \", \n",
        "  \" TR \"  :  \" DEANNA TROI \", \n",
        "  \" EM \"  :  \" EMIL \", \n",
        "  \" EN \"  :  \" EMERGENCY NAVIGATIONAL \",\n",
        "  \" EH \"  :  \" EMERGENCY HOSPITALITY \",\n",
        "  \" EMM \" :  \" EMMOT \", \"'re\"  : ' are', \n",
        "  \" AR \" :   \"ARCANA \", \"'ve\" : \" have\",\n",
        "  \" ALT \" :  \"ALTON SOONG \", \"'nt\" : \" not \",\n",
        "  \" ST \"  :  \" SUTRA \",  \"'ll\":' will',\n",
        "  \" CL \"  :  \" ADMIRAL CLANCY \",\n",
        "  \"'S\" : '', ':' : '', ';' : '',\n",
        "  \"'\" : '', ' BILL ': 'BILLUPS',\n",
        "  ' MAR ' : ' BECKETT MARINER ', \n",
        "  ' RUTH ' : ' RUTHERFORD ',\n",
        "  ' V CAP ' : ' VANCOUVER CAPTAIN ',\n",
        "  ' CAP ' : 'CAPTAIN CAROL FREEMAN',\n",
        "  ' RAN ' : ' JACK RANSOM',\n",
        "  ' TEN ' : ' DVANA TENDI ',\n",
        "  ' SHAX ' : 'SHAXS', \n",
        "  ' PHAMP ' : ' PEANUT HAMPER ',\n",
        "  ' BARB ' : ' BARBARA BRINSON ',\n",
        "  ' DOC ' : ' RON DOCENT ', \n",
        "  ' MIXTUS CIT: ' : ' MIXTUS CITIZEN', \n",
        "  ' AMI ' : ' CAPTAIN AMINA ', \n",
        "  ' DUR ' : ' DURGAN ', \n",
        "  ' DIV ' : ' SPECIALIST DIVISION 14',\n",
        "  ' PRA ' : ' PRACHETT ',\n",
        "  ' WAR ' : ' WARREN ',\n",
        "  ' HALF ' : ' HALF-OLD HALF-YOUNG ',\n",
        "  ' INT. ' : ' INTERIOR ', \n",
        "  ' LT. ' : ' LIEUTENANT ',\n",
        "  ' LT ' : ' LIEUTENANT ',\n",
        "  ' CMR. ' : ' COMMANDER ',\n",
        "  ' CMR ' : ' COMMANDER ',\n",
        "  ' POL ' : ' DOCTOR POLLARD ',\n",
        "  ' DR.' : ' DOCTOR ',\n",
        "  ' starlog ' : ' log ',\n",
        "  ' EXT. ' : ' EXTERIOR '\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTBhMyCshCl9"
      },
      "source": [
        "to_drop = ['CONTINUED', 'CUT TO', 'FADE IN', \n",
        "           'FADE OUT', 'END OF TEASER', 'OMITTED', 'INTERCUT:', \n",
        "           'THE WRITING CREDITS MAY NOT BE FINAL',  'THE END',\n",
        "           'FINAL DRAFT', 'FIRST DRAFT', 'THEME MUSIC',\n",
        "           'END OF ACT', '/66', '/67', '/68', '/69', '/70', \n",
        "           '/71', '/72', '/73', '/74', '/75', '/76', '/77', \n",
        "           '/78', '/79', '/80', '/81', '/82', '/83', '/84', \n",
        "           '/85', '/86', '/87', '/88', '/89', '/90', '/91', \n",
        "           '/92', '/93', '/94', '/95', '/96', '/97', '/98', \n",
        "           '/99', '/00', '/01', '/02', '/03', '/04', '/05', \n",
        "           '/06', '/07', '/08', '/09', '/10', '/11', '/12',\n",
        "           '/13', '/14', '/15', '/16', '/17', '/18', '/19', \n",
        "           '/20', '/21', 'END OF ACT', 'STAR TREK:', 'SPACE NINE:', \n",
        "           'VOYAGER:', 'PRISE:', 'GENERATION:', 'ACT ONE', 'ACT TWO', \n",
        "           'ACT THREE', 'ACT FOUR', 'ACT FIVE', 'ACT SIX', 'ACT SEVEN', \n",
        "           'ACT EIGHT', 'SCENE CHANGE', 'BEAT.', 'AS BEFORE.', \n",
        "           'NOTE:', 'DISSOLVE TO:', 'PART ONE', 'PART TWO', 'PART THREE', \n",
        "           'PART FOUR', 'THRU OMITTED', '(BEAT)', '(CONTINUING)',\n",
        "           'FADE IN', 'FADE OUT', 'CUT TO', 'ANGLE ON',\n",
        "           'NEW ANGLE', 'ANGLE EMPHASIZING', '(A BEAT)',\n",
        "           ' POV ', 'PART ONE', 'PART TWO', 'PART THREE', 'PART FOUR',\n",
        "           'ACT ONE', 'ACT TWO', 'ACT THREE', 'ACT FOUR', 'TEASER']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAriFuCnG84G"
      },
      "source": [
        "def finetune(sess,\n",
        "             dataset,\n",
        "             steps=-1,\n",
        "             model_name='124M',\n",
        "             model_dir='models',\n",
        "             combine=50000,\n",
        "             batch_size=1,\n",
        "             learning_rate=0.0001,\n",
        "             accumulate_gradients=5,\n",
        "             restore_from='latest',\n",
        "             run_name='run1',\n",
        "             checkpoint_dir='checkpoint',\n",
        "             sample_every=100,\n",
        "             sample_length=1023,\n",
        "             sample_num=1,\n",
        "             multi_gpu=False,\n",
        "             save_every=1000,\n",
        "             print_every=1,\n",
        "             max_checkpoints=1,\n",
        "             use_memory_saving_gradients=False,\n",
        "             only_train_transformer_layers=False,\n",
        "             optimizer='adam',\n",
        "             overwrite=False):\n",
        "    \"\"\"Finetunes the model on the given dataset.\n",
        "\n",
        "    Adapted from https://github.com/nshepperd/gpt-2/blob/finetuning/train.py.\n",
        "    See that file for parameter definitions.\n",
        "    \"\"\"\n",
        "    SAMPLE_DIR = 'samples'\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
        "\n",
        "    def maketree(path):\n",
        "        try:\n",
        "            os.makedirs(path)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    maketree(checkpoint_path)\n",
        "    files = [f for f in os.listdir(checkpoint_path)]\n",
        "    for file in ['hparams.json', 'encoder.json', 'vocab.bpe']:\n",
        "        try:\n",
        "            shutil.copyfile(os.path.join(model_dir, model_name, file),\n",
        "                            os.path.join(checkpoint_path, file))\n",
        "        except FileNotFoundError as fnf_error:\n",
        "            print(\"You need to download the GPT-2 model first via download_gpt2()\")\n",
        "            raise(fnf_error)\n",
        "\n",
        "    enc = encoder.get_encoder(checkpoint_path)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if sample_length > hparams.n_ctx:\n",
        "        raise ValueError(\n",
        "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    if model_name not in ['117M', '124M']:\n",
        "        use_memory_saving_gradients = True\n",
        "        only_train_transformer_layers = True\n",
        "        accumulate_gradients = 1\n",
        "\n",
        "    context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
        "    gpus = []\n",
        "\n",
        "    if multi_gpu:\n",
        "        gpus = get_available_gpus()\n",
        "\n",
        "    output = model.model(hparams=hparams, X=context, gpus=gpus)\n",
        "    loss = tf.reduce_mean(\n",
        "        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=context[:, 1:], logits=output['logits'][:, :-1]))\n",
        "\n",
        "    tf_sample = sample.sample_sequence(\n",
        "        hparams=hparams,\n",
        "        length=sample_length,\n",
        "        context=context,\n",
        "        batch_size=batch_size,\n",
        "        temperature=1.0,\n",
        "        top_k=40)\n",
        "\n",
        "    all_vars = [v for v in tf.compat.v1.trainable_variables() if 'model' in v.name]\n",
        "    train_vars = [v for v in all_vars if '/h' in v.name] if only_train_transformer_layers else all_vars\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "    if accumulate_gradients > 1:\n",
        "        if use_memory_saving_gradients:\n",
        "            exit(\"Memory saving gradients are not implemented for gradient accumulation yet.\")\n",
        "        opt = AccumulatingOptimizer(\n",
        "            opt=opt,\n",
        "            var_list=train_vars)\n",
        "        opt_reset = opt.reset()\n",
        "        opt_compute = opt.compute_gradients(loss)\n",
        "        opt_apply = opt.apply_gradients()\n",
        "        summary_loss = tf.compat.v1.summary.scalar('loss', opt_apply)\n",
        "    else:\n",
        "        if use_memory_saving_gradients:\n",
        "            opt_grads = memory_saving_gradients.gradients(loss, train_vars)\n",
        "        else:\n",
        "            opt_grads = tf.gradients(ys=loss, xs=train_vars)\n",
        "        opt_grads = list(zip(opt_grads, train_vars))\n",
        "        opt_apply = opt.apply_gradients(opt_grads)\n",
        "        summary_loss = tf.compat.v1.summary.scalar('loss', loss)\n",
        "\n",
        "    summary_log = tf.compat.v1.summary.FileWriter(checkpoint_path)\n",
        "\n",
        "    saver = tf.compat.v1.train.Saver(\n",
        "        var_list=all_vars,\n",
        "        max_to_keep=max_checkpoints)\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    if restore_from == 'latest':\n",
        "        ckpt = tf.train.latest_checkpoint(checkpoint_path)\n",
        "        if ckpt is None:\n",
        "            # Get fresh GPT weights if new run.\n",
        "            ckpt = tf.train.latest_checkpoint(\n",
        "                os.path.join(model_dir, model_name))\n",
        "    elif restore_from == 'fresh':\n",
        "        ckpt = tf.train.latest_checkpoint(\n",
        "            os.path.join(model_dir, model_name))\n",
        "    else:\n",
        "        ckpt = tf.train.latest_checkpoint(restore_from)\n",
        "    print('Loading checkpoint', ckpt)\n",
        "    saver.restore(sess, ckpt)\n",
        "\n",
        "    print('Loading dataset...')\n",
        "    chunks = load_dataset(enc, dataset, combine)\n",
        "    data_sampler = Sampler(chunks)\n",
        "    print('dataset has', data_sampler.total_size, 'tokens')\n",
        "    print('Training...')\n",
        "\n",
        "    counter = 1\n",
        "    counter_path = os.path.join(checkpoint_path, 'counter')\n",
        "    if os.path.exists(counter_path) and restore_from == 'latest':\n",
        "        # Load the step number if we're resuming a run\n",
        "        # Add 1 so we don't immediately try to save again\n",
        "        with open(counter_path, 'r') as fp:\n",
        "            counter = int(fp.read()) + 1\n",
        "    counter_base = counter\n",
        "\n",
        "    def save():\n",
        "        maketree(checkpoint_path)\n",
        "        print(\n",
        "            'Saving',\n",
        "            os.path.join(checkpoint_path,\n",
        "                         'model-{}').format(counter-1))\n",
        "        saver.save(\n",
        "            sess,\n",
        "            os.path.join(checkpoint_path, 'model'),\n",
        "            global_step=counter-1)\n",
        "        with open(counter_path, 'w') as fp:\n",
        "            fp.write(str(counter-1) + '\\n')\n",
        "\n",
        "    def generate_samples():\n",
        "        context_tokens = data_sampler.sample(1)\n",
        "        all_text = []\n",
        "        index = 0\n",
        "        while index < sample_num:\n",
        "            out = sess.run(\n",
        "                tf_sample,\n",
        "                feed_dict={context: batch_size * [context_tokens]})\n",
        "            for i in range(min(sample_num - index, batch_size)):\n",
        "                text = enc.decode(out[i])\n",
        "                text = '======== SAMPLE {} ========\\n{}\\n'.format(\n",
        "                    index + 1, text)\n",
        "                all_text.append(text)\n",
        "                index += 1\n",
        "        print(text)\n",
        "        maketree(os.path.join(SAMPLE_DIR, run_name))\n",
        "        with open(\n",
        "                os.path.join(SAMPLE_DIR, run_name,\n",
        "                             'samples-{}').format(counter), 'w') as fp:\n",
        "            fp.write('\\n'.join(all_text))\n",
        "\n",
        "    def sample_batch():\n",
        "        return [data_sampler.sample(1024) for _ in range(batch_size)]\n",
        "\n",
        "    if overwrite and restore_from == 'latest':\n",
        "        for file in files:\n",
        "            if file.startswith('model') or file.startswith('events'):\n",
        "                os.remove(os.path.join(checkpoint_path, file))\n",
        "        save()\n",
        "\n",
        "    avg_loss = (0.0, 0.0)\n",
        "    start_time = time.time()\n",
        "\n",
        "    if steps:\n",
        "        steps = int(steps)\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            if steps > 0 and counter == (counter_base + steps):\n",
        "                save()\n",
        "                return\n",
        "            if (counter - 1) % save_every == 0 and counter > 1:\n",
        "                save()\n",
        "            if (counter - 1) % sample_every == 0 and counter > 1:\n",
        "                generate_samples()\n",
        "\n",
        "            if accumulate_gradients > 1:\n",
        "                sess.run(opt_reset)\n",
        "                for _ in range(accumulate_gradients):\n",
        "                    sess.run(\n",
        "                        opt_compute, feed_dict={context: sample_batch()})\n",
        "                (v_loss, v_summary) = sess.run((opt_apply, summary_loss))\n",
        "            else:\n",
        "                (_, v_loss, v_summary) = sess.run(\n",
        "                    (opt_apply, loss, summary_loss),\n",
        "                    feed_dict={context: sample_batch()})\n",
        "\n",
        "            summary_log.add_summary(v_summary, counter)\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                avg_loss = (avg_loss[0] * 0.99 + v_loss,\n",
        "                            avg_loss[1] * 0.99 + 1.0)\n",
        "                avg=avg_loss[0] / avg_loss[1]\n",
        "                if avg  < 1.50:\n",
        "                  save()\n",
        "                  gpt2.copy_checkpoint_to_gdrive(run_name=run_name)\n",
        "                  print('Model Trained.')\n",
        "                  break\n",
        "                print(\n",
        "                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
        "                    .format(\n",
        "                        counter=counter,\n",
        "                        time=time.time() - start_time,\n",
        "                        loss=v_loss,\n",
        "                        avg=avg_loss[0] / avg_loss[1]))\n",
        "\n",
        "            counter += 1\n",
        "    except KeyboardInterrupt:\n",
        "        print('interrupted')\n",
        "        save()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BBj5zowNx0d"
      },
      "source": [
        "def open_scripts(start, end):\n",
        "  \"\"\"\n",
        "  Opens and seperates scripts into 4 equal parts for training. \n",
        "  Removes corrupted bytes and minorly scrubs data. \n",
        "  \"\"\"\n",
        "  a = ''\n",
        "  b = ''\n",
        "  c = ''\n",
        "  d = ''\n",
        "  rng = [i for i in range(start, end)]\n",
        "  for x in [15, 354, 377, 378, 379, 380, 381, 382, 383, 384, 385, \n",
        "            386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, \n",
        "            397, 398, 399, 400, 401, 473, 610, 718]:\n",
        "    if x in rng:\n",
        "      rng.remove(x)\n",
        "  txts = ''\n",
        "  for i in rng:\n",
        "    txt = open(f\"/content/drive/MyDrive/trekgenerator/{i}.txt\",\n",
        "                \"r\", encoding='utf-8', errors='ignore')\n",
        "    txt = txt.read()\n",
        "    txt = txt.replace(\"\\t\", \" \").replace('...', ' ').replace('..', ' ')\n",
        "    txt = txt.replace('                                           ',\n",
        "                      ' ').replace('\\\\', '').replace('--', '-')\n",
        "    txt = txt.replace('       ', ' ').strip().replace('      ', \n",
        "                      ' ').replace('     ', ' ')\n",
        "    txt = txt.replace('    ', ' ').replace('   ',\n",
        "                      ' ').replace('   ', ' ').replace('  ', ' ')\n",
        "    if 'PART ONE' in txt:\n",
        "      txt = txt[txt.find('PART ONE'):]\n",
        "      part_one = txt[txt.find('PART ONE'):txt.find('PART TWO')]\n",
        "      part_two = txt[txt.find('PART TWO'):txt.find('PART THREE')]\n",
        "      part_three = txt[txt.find('PART THREE'):txt.find('PART FOUR')]\n",
        "      part_four = txt[txt.find('PART FOUR'):]\n",
        "    elif 'PART FIVE' in txt:\n",
        "      txt = txt[txt.find('PART FIVE'):]\n",
        "      part_one = txt[txt.find('PART FIVE'):txt.find('PART SIX')]\n",
        "      part_two = txt[txt.find('PART SIX'):txt.find('PART SEVEN')]\n",
        "      part_three = txt[txt.find('PART SEVEN'):txt.find('PART EIGHT')]\n",
        "      part_four = txt[txt.find('PART EIGHT'):]\n",
        "    elif 'ACT ONE' in txt:\n",
        "      txt = txt[txt.find('ACT ONE'):]\n",
        "      part_one = txt[txt.find('ACT ONE'):txt.find('ACT TWO')]\n",
        "      part_two = txt[txt.find('ACT TWO'):txt.find('ACT THREE')]\n",
        "      part_three = txt[txt.find('ACT THREE'):txt.find('ACT FOUR')]\n",
        "      part_four = txt[txt.find('ACT FOUR'):]\n",
        "    elif 'ACT FIVE' in txt:\n",
        "      txt = txt[txt.find('ACT FIVE'):]\n",
        "      part_one = txt[txt.find('ACT FIVE'):txt.find('ACT SIX')]\n",
        "      part_two = txt[txt.find('ACT SIX'):txt.find('ACT SEVEN')]\n",
        "      part_three = txt[txt.find('ACT SEVEN'):txt.find('ACT EIGHT')]\n",
        "      part_four = txt[txt.find('ACT EIGHT'):]\n",
        "    a += part_one.strip() + ' <|endoftext|> \\n <|startoftext|> '\n",
        "    b += part_two.strip() + ' <|endoftext|> \\n <|startoftext|> '\n",
        "    c += part_three.strip() + ' <|endoftext|> \\n <|startoftext|> '\n",
        "    d += part_four.strip() + ' <|endoftext|> \\n <|startoftext|> '\n",
        "    txts += txt.strip() + ' <|endoftext|> \\n <|startoftext|> '\n",
        "  return a, b, c, d, txts \n",
        "a, b, c, d , txts = open_scripts(0, 826)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47PdKh3oOBMa"
      },
      "source": [
        "for letter in [a, b, c, d]:\n",
        "  letter = letter.replace('+', ' ').replace('\\x00', '').replace('\\n', ' \\n ').strip()\n",
        "  letter = letter.replace('[', '(').replace(']', ')').replace('{', '(').replace('}', ')')\n",
        "  letter = letter.replace('((', '(').replace('))', ')')[:-17]\n",
        "  for key in names.keys():\n",
        "    letter = letter.replace(key, names[key])\n",
        "  letter = letter.split('\\n')\n",
        "  letter = pd.DataFrame(letter)\n",
        "  letter = letter.loc[letter[0] != '']\n",
        "  letter = letter.rename(columns={0: 'quote'})\n",
        "  letter.reset_index(inplace=True, drop=True)\n",
        "  letter = letter[letter['quote'] != ':']\n",
        "  letter = letter[letter['quote'] != '-']\n",
        "  letter = letter[letter['quote'] != '\\n']\n",
        "  letter = letter[letter['quote'] != '.']\n",
        "  letter = letter.loc[letter['quote'] != '']\n",
        "  letter = letter.loc[letter['quote'] != ' ']\n",
        "  prev = 0 \n",
        "  letter.reset_index(inplace=True, drop=True)\n",
        "  for i in range(len(letter)):\n",
        "    string = ''\n",
        "    bool1 = any(ext in letter.at[i, 'quote'] for ext in to_drop)\n",
        "    bool2 = letter.at[i, 'quote'].replace('\\n', '').strip()[:-1].isnumeric()\n",
        "    bool3 = len(letter.at[i, 'quote']) < 6\n",
        "    if bool1 or bool2 or bool3:\n",
        "      letter.drop(index=i, inplace = True)\n",
        "    else:\n",
        "      letter.at[i, 'quote'] = letter.at[i, 'quote'].replace('\\n', '').strip()\n",
        "      caps = letter.at[i, 'quote'].isupper()\n",
        "      sentence = letter.at[i, 'quote'].split()\n",
        "      num = 0\n",
        "      for j in sentence:\n",
        "        if j.isupper() and num <= 2 and j.startswith('(') == False:\n",
        "          string += j + ' '\n",
        "          num += 1\n",
        "        else:\n",
        "          string += j.lower() + ' '\n",
        "          num += 1\n",
        "      if '(FORMERLY' in string:\n",
        "        string = string[:string.find('(FORMERLY')]\n",
        "      end = letter.at[i, 'quote'].strip()[-1:]\n",
        "      if end in ['.', '?', '(', ')', '!'] or caps and len(sentence) >= 2:\n",
        "        end = ' \\n '\n",
        "      else:\n",
        "        end = ''\n",
        "      letter.at[i, 'quote'] = string + end\n",
        "  letter.reset_index(inplace=True, drop=True)\n",
        "  A = ''\n",
        "  for i in a.index:\n",
        "    quote = letter.at[i, \"quote\"]\n",
        "    while quote[:1].isnumeric() or quote[:1] == '-' or quote[:1] == '.':\n",
        "      quote = quote[1:]\n",
        "    for i in quote:\n",
        "      if i.isalnum() == False:\n",
        "        if i not in [',', '.', '?', '(', ')', '-', '!']:\n",
        "          quote.replace(f'{i}', '')\n",
        "    quote.replace('---', '').replace('--', '')\n",
        "    A += quote + ' '\n",
        "  A = A.replace('--', '').replace(' \\n  \\n ', ' \\n ').replace('  ', ' ')\n",
        "  with open(f'/content/drive/MyDrive{letter}.txt', 'w') as f:\n",
        "      f.write(A)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}