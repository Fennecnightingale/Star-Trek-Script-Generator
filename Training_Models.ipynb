{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training Models",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2UCEi5RtkjF"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        " \n",
        "import re \n",
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import tarfile\n",
        "import requests\n",
        "import argparse\n",
        " \n",
        " \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gpt_2_simple as gpt2\n",
        " \n",
        "from tqdm import tqdm, trange\n",
        "from datetime import datetime\n",
        " \n",
        "from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "from tensorflow.python.client import device_lib\n",
        " \n",
        "from gpt_2_simple.src import model, sample, encoder, memory_saving_gradients\n",
        "from gpt_2_simple.src.load_dataset import load_dataset, Sampler\n",
        "from gpt_2_simple.src.accumulate import AccumulatingOptimizer\n",
        " \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBAJUYcKtvPk"
      },
      "source": [
        "def finetune(sess,\n",
        "             dataset,\n",
        "             steps=-1,\n",
        "             model_name='124M',\n",
        "             model_dir='models',\n",
        "             combine=50000,\n",
        "             batch_size=1,\n",
        "             learning_rate=0.0001,\n",
        "             accumulate_gradients=5,\n",
        "             restore_from='latest',\n",
        "             run_name='run1',\n",
        "             checkpoint_dir='checkpoint',\n",
        "             sample_every=100,\n",
        "             sample_length=1023,\n",
        "             sample_num=1,\n",
        "             multi_gpu=False,\n",
        "             save_every=1000,\n",
        "             print_every=1,\n",
        "             max_checkpoints=1,\n",
        "             use_memory_saving_gradients=False,\n",
        "             only_train_transformer_layers=False,\n",
        "             optimizer='adam',\n",
        "             overwrite=False, \n",
        "             train_to=1.50):\n",
        "    \"\"\"Finetunes the model on the given dataset.\n",
        "\n",
        "    Adapted from https://github.com/nshepperd/gpt-2/blob/finetuning/train.py.\n",
        "    See that file for parameter definitions.\n",
        "    Additonal changes by Fennec are limited to adding an end point for training,\n",
        "    and forcing it to save to Google Drive more regularly for less lost progress\n",
        "    due to random timeouts. \n",
        "    \"\"\"\n",
        "    SAMPLE_DIR = 'samples'\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
        "\n",
        "    def maketree(path):\n",
        "        try:\n",
        "            os.makedirs(path)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    maketree(checkpoint_path)\n",
        "    files = [f for f in os.listdir(checkpoint_path)]\n",
        "    for file in ['hparams.json', 'encoder.json', 'vocab.bpe']:\n",
        "        try:\n",
        "            shutil.copyfile(os.path.join(model_dir, model_name, file),\n",
        "                            os.path.join(checkpoint_path, file))\n",
        "        except FileNotFoundError as fnf_error:\n",
        "            print(\"You need to download the GPT-2 model first via download_gpt2()\")\n",
        "            raise(fnf_error)\n",
        "\n",
        "    enc = encoder.get_encoder(checkpoint_path)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if sample_length > hparams.n_ctx:\n",
        "        raise ValueError(\n",
        "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    if model_name not in ['117M', '124M']:\n",
        "        use_memory_saving_gradients = True\n",
        "        only_train_transformer_layers = True\n",
        "        accumulate_gradients = 1\n",
        "\n",
        "    context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
        "    gpus = []\n",
        "\n",
        "    if multi_gpu:\n",
        "        gpus = get_available_gpus()\n",
        "\n",
        "    output = model.model(hparams=hparams, X=context, gpus=gpus)\n",
        "    loss = tf.reduce_mean(\n",
        "        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=context[:, 1:], logits=output['logits'][:, :-1]))\n",
        "\n",
        "    tf_sample = sample.sample_sequence(\n",
        "        hparams=hparams,\n",
        "        length=sample_length,\n",
        "        context=context,\n",
        "        batch_size=batch_size,\n",
        "        temperature=1.0,\n",
        "        top_k=40)\n",
        "\n",
        "    all_vars = [v for v in tf.compat.v1.trainable_variables() if 'model' in v.name]\n",
        "    train_vars = [v for v in all_vars if '/h' in v.name] if only_train_transformer_layers else all_vars\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "    if accumulate_gradients > 1:\n",
        "        if use_memory_saving_gradients:\n",
        "            exit(\"Memory saving gradients are not implemented for gradient accumulation yet.\")\n",
        "        opt = AccumulatingOptimizer(\n",
        "            opt=opt,\n",
        "            var_list=train_vars)\n",
        "        opt_reset = opt.reset()\n",
        "        opt_compute = opt.compute_gradients(loss)\n",
        "        opt_apply = opt.apply_gradients()\n",
        "        summary_loss = tf.compat.v1.summary.scalar('loss', opt_apply)\n",
        "    else:\n",
        "        if use_memory_saving_gradients:\n",
        "            opt_grads = memory_saving_gradients.gradients(loss, train_vars)\n",
        "        else:\n",
        "            opt_grads = tf.gradients(ys=loss, xs=train_vars)\n",
        "        opt_grads = list(zip(opt_grads, train_vars))\n",
        "        opt_apply = opt.apply_gradients(opt_grads)\n",
        "        summary_loss = tf.compat.v1.summary.scalar('loss', loss)\n",
        "\n",
        "    summary_log = tf.compat.v1.summary.FileWriter(checkpoint_path)\n",
        "\n",
        "    saver = tf.compat.v1.train.Saver(\n",
        "        var_list=all_vars,\n",
        "        max_to_keep=max_checkpoints)\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    if restore_from == 'latest':\n",
        "        ckpt = tf.train.latest_checkpoint(checkpoint_path)\n",
        "        if ckpt is None:\n",
        "            # Get fresh GPT weights if new run.\n",
        "            ckpt = tf.train.latest_checkpoint(\n",
        "                os.path.join(model_dir, model_name))\n",
        "    elif restore_from == 'fresh':\n",
        "        ckpt = tf.train.latest_checkpoint(\n",
        "            os.path.join(model_dir, model_name))\n",
        "    else:\n",
        "        ckpt = tf.train.latest_checkpoint(restore_from)\n",
        "    print('Loading checkpoint', ckpt)\n",
        "    saver.restore(sess, ckpt)\n",
        "\n",
        "    print('Loading dataset...')\n",
        "    chunks = load_dataset(enc, dataset, combine)\n",
        "    data_sampler = Sampler(chunks)\n",
        "    print('dataset has', data_sampler.total_size, 'tokens')\n",
        "    print('Training...')\n",
        "\n",
        "    counter = 1\n",
        "    counter_path = os.path.join(checkpoint_path, 'counter')\n",
        "    if os.path.exists(counter_path) and restore_from == 'latest':\n",
        "        # Load the step number if we're resuming a run\n",
        "        # Add 1 so we don't immediately try to save again\n",
        "        with open(counter_path, 'r') as fp:\n",
        "            counter = int(fp.read()) + 1\n",
        "    counter_base = counter\n",
        "\n",
        "    def save():\n",
        "        maketree(checkpoint_path)\n",
        "        print(\n",
        "            'Saving',\n",
        "            os.path.join(checkpoint_path,\n",
        "                         'model-{}').format(counter-1))\n",
        "        saver.save(\n",
        "            sess,\n",
        "            os.path.join(checkpoint_path, 'model'),\n",
        "            global_step=counter-1)\n",
        "        with open(counter_path, 'w') as fp:\n",
        "            fp.write(str(counter-1) + '\\n')\n",
        "\n",
        "    def generate_samples():\n",
        "        context_tokens = data_sampler.sample(1)\n",
        "        all_text = []\n",
        "        index = 0\n",
        "        while index < sample_num:\n",
        "            out = sess.run(\n",
        "                tf_sample,\n",
        "                feed_dict={context: batch_size * [context_tokens]})\n",
        "            for i in range(min(sample_num - index, batch_size)):\n",
        "                text = enc.decode(out[i])\n",
        "                text = '======== SAMPLE {} ========\\n{}\\n'.format(\n",
        "                    index + 1, text)\n",
        "                all_text.append(text)\n",
        "                index += 1\n",
        "        print(text)\n",
        "        maketree(os.path.join(SAMPLE_DIR, run_name))\n",
        "        with open(\n",
        "                os.path.join(SAMPLE_DIR, run_name,\n",
        "                             'samples-{}').format(counter), 'w') as fp:\n",
        "            fp.write('\\n'.join(all_text))\n",
        "\n",
        "    def sample_batch():\n",
        "        return [data_sampler.sample(1024) for _ in range(batch_size)]\n",
        "\n",
        "    if overwrite and restore_from == 'latest':\n",
        "        for file in files:\n",
        "            if file.startswith('model') or file.startswith('events'):\n",
        "                os.remove(os.path.join(checkpoint_path, file))\n",
        "        save()\n",
        "\n",
        "    avg_loss = (0.0, 0.0)\n",
        "    start_time = time.time()\n",
        "\n",
        "    if steps:\n",
        "        steps = int(steps)\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            if steps > 0 and counter == (counter_base + steps):\n",
        "                save()\n",
        "                gpt2.copy_checkpoint_to_gdrive(run_name=run_name)\n",
        "                return\n",
        "            if (counter - 1) % save_every == 0 and counter > 1:\n",
        "                save()\n",
        "                gpt2.copy_checkpoint_to_gdrive(run_name=run_name)\n",
        "            if (counter - 1) % sample_every == 0 and counter > 1:\n",
        "                generate_samples()\n",
        "                gpt2.copy_checkpoint_to_gdrive(run_name=run_name)\n",
        "\n",
        "            if accumulate_gradients > 1:\n",
        "                sess.run(opt_reset)\n",
        "                for _ in range(accumulate_gradients):\n",
        "                    sess.run(\n",
        "                        opt_compute, feed_dict={context: sample_batch()})\n",
        "                (v_loss, v_summary) = sess.run((opt_apply, summary_loss))\n",
        "            else:\n",
        "                (_, v_loss, v_summary) = sess.run(\n",
        "                    (opt_apply, loss, summary_loss),\n",
        "                    feed_dict={context: sample_batch()})\n",
        "\n",
        "            summary_log.add_summary(v_summary, counter)\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                avg_loss = (avg_loss[0] * 0.99 + v_loss,\n",
        "                            avg_loss[1] * 0.99 + 1.0)\n",
        "                avg=avg_loss[0] / avg_loss[1]\n",
        "                if avg  < train_to:\n",
        "                  save()\n",
        "                  gpt2.copy_checkpoint_to_gdrive(run_name=run_name)\n",
        "                  print('Model Trained.')\n",
        "                  break\n",
        "                print(\n",
        "                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
        "                    .format(\n",
        "                        counter=counter,\n",
        "                        time=time.time() - start_time,\n",
        "                        loss=v_loss,\n",
        "                        avg=avg_loss[0] / avg_loss[1]))\n",
        "\n",
        "            counter += 1\n",
        "    except KeyboardInterrupt:\n",
        "        print('interrupted')\n",
        "        save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3Y6eiKxuFy8"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"774M\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rralAqQyt7Yr"
      },
      "source": [
        "models = ['old_d']\n",
        "\n",
        "for i in models: \n",
        "  file_name = f\"{i}.all.txt\"\n",
        "  try: \n",
        "  # start or restart the session\n",
        "    if bool(sess):\n",
        "      sess = gpt2.reset_session(sess)\n",
        "  except:\n",
        "    sess = gpt2.start_tf_sess()\n",
        "  # copy the text file over from google drive \n",
        "  gpt2.copy_file_from_gdrive(file_name)\n",
        "  # if we've started training this model already get the last save point \n",
        "  try:\n",
        "    gpt2.copy_checkpoint_from_gdrive(run_name=i)\n",
        "  except:\n",
        "  # if not start a fresh one \n",
        "    pass\n",
        "  # start finetuning \n",
        "  finetune(sess,\n",
        "          dataset = file_name,\n",
        "          model_name = '774M',\n",
        "          steps = -1,\n",
        "          learning_rate = 0.00025,\n",
        "          restore_from = 'latest',\n",
        "          overwrite = True,\n",
        "          run_name = i,\n",
        "          save_every = 100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}